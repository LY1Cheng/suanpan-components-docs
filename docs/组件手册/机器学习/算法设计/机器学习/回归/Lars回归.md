# Lars回归使用文档
| 组件名称 | Lars回归|  |  |
| --- | --- | --- | --- |
| 工具集 | 机器学习 |  |  |
| 组件作者 | 雪浪云-墨文 |  |  |
| 文档版本 | 1.0 |  |  |
| 功能 | Lars回归算法|  |  |
| 镜像名称 | ml_components:3 |  |  |
| 开发语言 | Python |  |  |

## 组件原理
在统计学中，最小角度回归(LARS)是一种将线性回归模型拟合到高维空间的算法，由Bradley Efron、Trevor Hastie、Iain Johnstone和Robert Tibshirani开发。

假设我们期望一个响应变量由一个可能的协变量子集的线性组合决定。然后，LARS算法提供了一种方法来生成要包括哪些变量及其系数的估计值。

LARS解由一条曲线组成，表示参数L1范数参数向量的每个值的解，而不是给出向量结果。

LARS的算法实际执行步骤如下：

- 对Predictors进行标准化（去除不同尺度的影响），对Target Variable进行中心化（去除截距项的影响），初始的所有系数都设为0，此时残差 r 就等于中心化后的Target Variable
- 找出和残差r相关度最高的变量X_j
- 将X_j的系数Beta_j 从0开始沿着LSE（只有一个变量X_j的最小二乘估计）的方向变化，直到某个新的变量X_k与残差r的相关性大于X_j时
-  X_j和X_k的系数Beta_j和Beta_k，一起沿着新的LSE（加入了新变量X_k的最小二乘估计）的方向移动，直到有新的变量被选入
- 重复2，3，4，直到所有变量被选入，最后得到的估计就是普通线性回归的OLS

从上面这个算法可以看出，LARS这个东西明显和OLS, Ridge Regression等给出了Closed-form solutions的模型不同，而是给出了一套对计算机来说非常友好的算法。这也说明了随着计算机能力的强大，现代统计基本上越来越靠近算法，而和模型无关。

![](./img/Lars回归1.png)
## 输入桩
支持单个csv文件输入。
### 输入端子1

- **端口名称**：训练数据
- **输入类型**：Csv文件
- **功能描述**： 输入用于训练的数据
## 输出桩
支持sklearn模型输出。
### 输出端子1

- **端口名称**：输出模型
- **输出类型**：sklearn模型
- **功能描述**： 输出训练好的模型用于预测
## 参数配置
### Fit Intercept

- **功能描述**:是否计算该模型的截距。
- **必选参数**：是
- **默认值**：true
### 是否进行归一化

- **功能描述**：是否对数据进行归一化处理，该参数在Fit Intercept参数设为False时会被忽略。
- **必选参数**：是
- **默认值**：true
### precompute

- **功能描述**:是否使用预先计算好的Gram矩阵来加速计算。
- **必选参数**：是
- **默认值**：auto
### N Nonzero Coefs

- **功能描述**:非零系数的目标数。
- **必选参数**：是
- **默认值**：500
### eps

- **功能描述**:计算Cholesky对角因子的机器精度正则化。对于条件很差的系统，增加这个值。
- **必选参数**：否
- **默认值**：（无）
### jitter

- **功能描述**：要添加到y值的均匀噪声参数的上限，以满足模型一次一个计算的假设。可能有助于模型的稳定性
- **必选参数**：否
- **默认值**：（无）
### 需要训练

- **功能描述**：该模型是否需要训练，默认为需要训练。
- **必选参数**：是
- **默认值**：true
### 特征字段

- **功能描述**：特征字段
- **必选参数**：是
- **默认值**：（无）
### 识别字段

- **功能描述**：识别字段
- **必选参数**：是
- **默认值**：（无）
## 使用方法
- 将组件拖入到项目中
- 与前一个组件输出的端口连接（必须是csv类型）
- 点击运行该节点


![](./img/Lars回归2.png)



